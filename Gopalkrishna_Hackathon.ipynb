{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hackathon2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tihiqv_vFKWZ",
        "colab_type": "text"
      },
      "source": [
        "The Following Code works on Google Colab and uploading on Tweets.csv file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKsmjwaJ25WO",
        "colab_type": "text"
      },
      "source": [
        "1.Import the libraries, load dataset, print shape of data, data description.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1S43ZnxLLGkd",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kJ1BRKnWLUUi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "outputId": "367ce397-5283-4551-e238-346007626a9e"
      },
      "source": [
        "data = pd.read_csv('Tweets.csv')\n",
        "data.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>airline_sentiment</th>\n",
              "      <th>airline_sentiment_confidence</th>\n",
              "      <th>negativereason</th>\n",
              "      <th>negativereason_confidence</th>\n",
              "      <th>airline</th>\n",
              "      <th>airline_sentiment_gold</th>\n",
              "      <th>name</th>\n",
              "      <th>negativereason_gold</th>\n",
              "      <th>retweet_count</th>\n",
              "      <th>text</th>\n",
              "      <th>tweet_coord</th>\n",
              "      <th>tweet_created</th>\n",
              "      <th>tweet_location</th>\n",
              "      <th>user_timezone</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>570306133677760513</td>\n",
              "      <td>neutral</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>NaN</td>\n",
              "      <td>cairdin</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica What @dhepburn said.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-24 11:35:52 -0800</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Eastern Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>570301130888122368</td>\n",
              "      <td>positive</td>\n",
              "      <td>0.3486</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>NaN</td>\n",
              "      <td>jnardino</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-24 11:15:59 -0800</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Pacific Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>570301083672813571</td>\n",
              "      <td>neutral</td>\n",
              "      <td>0.6837</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>NaN</td>\n",
              "      <td>yvonnalynn</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-24 11:15:48 -0800</td>\n",
              "      <td>Lets Play</td>\n",
              "      <td>Central Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>570301031407624196</td>\n",
              "      <td>negative</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>Bad Flight</td>\n",
              "      <td>0.7033</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>NaN</td>\n",
              "      <td>jnardino</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-24 11:15:36 -0800</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Pacific Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>570300817074462722</td>\n",
              "      <td>negative</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>Can't Tell</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>NaN</td>\n",
              "      <td>jnardino</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-24 11:14:45 -0800</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Pacific Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             tweet_id  ...               user_timezone\n",
              "0  570306133677760513  ...  Eastern Time (US & Canada)\n",
              "1  570301130888122368  ...  Pacific Time (US & Canada)\n",
              "2  570301083672813571  ...  Central Time (US & Canada)\n",
              "3  570301031407624196  ...  Pacific Time (US & Canada)\n",
              "4  570300817074462722  ...  Pacific Time (US & Canada)\n",
              "\n",
              "[5 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qkYjJARpMHND",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "329e809e-375d-4d66-9ee6-4c15588b1d6c"
      },
      "source": [
        "data.shape"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(14640, 15)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AAbeYyyMMRb2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "outputId": "1ff64d57-92d1-40df-8912-1c7b5d788a72"
      },
      "source": [
        "data.describe()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>airline_sentiment_confidence</th>\n",
              "      <th>negativereason_confidence</th>\n",
              "      <th>retweet_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1.464000e+04</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>10522.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>5.692184e+17</td>\n",
              "      <td>0.900169</td>\n",
              "      <td>0.638298</td>\n",
              "      <td>0.082650</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>7.791112e+14</td>\n",
              "      <td>0.162830</td>\n",
              "      <td>0.330440</td>\n",
              "      <td>0.745778</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>5.675883e+17</td>\n",
              "      <td>0.335000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>5.685592e+17</td>\n",
              "      <td>0.692300</td>\n",
              "      <td>0.360600</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>5.694779e+17</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.670600</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>5.698905e+17</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>5.703106e+17</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>44.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           tweet_id  ...  retweet_count\n",
              "count  1.464000e+04  ...   14640.000000\n",
              "mean   5.692184e+17  ...       0.082650\n",
              "std    7.791112e+14  ...       0.745778\n",
              "min    5.675883e+17  ...       0.000000\n",
              "25%    5.685592e+17  ...       0.000000\n",
              "50%    5.694779e+17  ...       0.000000\n",
              "75%    5.698905e+17  ...       0.000000\n",
              "max    5.703106e+17  ...      44.000000\n",
              "\n",
              "[8 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEhLInyv3jJw",
        "colab_type": "text"
      },
      "source": [
        "# **2.Understand of data-columns:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0QiUEZfWMU6E",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "0ed3503e-f455-47cc-bed0-e4fb3528c627"
      },
      "source": [
        "data.columns"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['tweet_id', 'airline_sentiment', 'airline_sentiment_confidence',\n",
              "       'negativereason', 'negativereason_confidence', 'airline',\n",
              "       'airline_sentiment_gold', 'name', 'negativereason_gold',\n",
              "       'retweet_count', 'text', 'tweet_coord', 'tweet_created',\n",
              "       'tweet_location', 'user_timezone'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10lpfZKh3xgl",
        "colab_type": "text"
      },
      "source": [
        "a.Drop all other columns except “text” and “airline_sentiment”."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BnWL_ftuMerD",
        "colab": {}
      },
      "source": [
        "data.drop(data.columns.difference(['text','airline_sentiment']), 1, inplace=True)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_01-yZKeM-nA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "45eab9eb-65da-4f94-b67a-eb4cf0245e93"
      },
      "source": [
        "data.shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(14640, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EbqticEeM8e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "855e8d37-112e-4eb2-f882-ef11f4fb5c60"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>airline_sentiment</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>neutral</td>\n",
              "      <td>@VirginAmerica What @dhepburn said.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>positive</td>\n",
              "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>neutral</td>\n",
              "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>negative</td>\n",
              "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>negative</td>\n",
              "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  airline_sentiment                                               text\n",
              "0           neutral                @VirginAmerica What @dhepburn said.\n",
              "1          positive  @VirginAmerica plus you've added commercials t...\n",
              "2           neutral  @VirginAmerica I didn't today... Must mean I n...\n",
              "3          negative  @VirginAmerica it's really aggressive to blast...\n",
              "4          negative  @VirginAmerica and it's a really big bad thing..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQL8gWX58VT9",
        "colab_type": "text"
      },
      "source": [
        "# **3. Text pre-processing: Data preparation.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "R6u0MXHnOAWI",
        "colab": {}
      },
      "source": [
        "# imports\n",
        "from bs4 import BeautifulSoup\n",
        "# function to remove HTML tags\n",
        "def remove_html_tags(text):\n",
        "    return BeautifulSoup(text, 'html.parser').get_text()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kZV4VSbkS2VH",
        "colab": {}
      },
      "source": [
        "# Rwmove html tags\n",
        "for i in range(len(data['text'])):\n",
        "  data['text'][i] = remove_html_tags(data['text'][i])"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OK3WAJRUTIiS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f5f7b3e6-cb57-49b1-d1dc-d65309511a55"
      },
      "source": [
        "#conda install -c anaconda nltk\n",
        "import nltk\n",
        "nltk.download()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> d\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> l\n",
            "Packages:\n",
            "  [ ] abc................. Australian Broadcasting Commission 2006\n",
            "  [ ] alpino.............. Alpino Dutch Treebank\n",
            "  [ ] averaged_perceptron_tagger Averaged Perceptron Tagger\n",
            "  [ ] averaged_perceptron_tagger_ru Averaged Perceptron Tagger (Russian)\n",
            "  [ ] basque_grammars..... Grammars for Basque\n",
            "  [ ] biocreative_ppi..... BioCreAtIvE (Critical Assessment of Information\n",
            "                           Extraction Systems in Biology)\n",
            "  [ ] bllip_wsj_no_aux.... BLLIP Parser: WSJ Model\n",
            "  [ ] book_grammars....... Grammars from NLTK Book\n",
            "  [ ] brown............... Brown Corpus\n",
            "  [ ] brown_tei........... Brown Corpus (TEI XML Version)\n",
            "  [ ] cess_cat............ CESS-CAT Treebank\n",
            "  [ ] cess_esp............ CESS-ESP Treebank\n",
            "  [ ] chat80.............. Chat-80 Data Files\n",
            "  [ ] city_database....... City Database\n",
            "  [ ] cmudict............. The Carnegie Mellon Pronouncing Dictionary (0.6)\n",
            "  [ ] comparative_sentences Comparative Sentence Dataset\n",
            "  [ ] comtrans............ ComTrans Corpus Sample\n",
            "  [ ] conll2000........... CONLL 2000 Chunking Corpus\n",
            "  [ ] conll2002........... CONLL 2002 Named Entity Recognition Corpus\n",
            "Hit Enter to continue: \n",
            "  [ ] conll2007........... Dependency Treebanks from CoNLL 2007 (Catalan\n",
            "                           and Basque Subset)\n",
            "  [ ] crubadan............ Crubadan Corpus\n",
            "  [ ] dependency_treebank. Dependency Parsed Treebank\n",
            "  [ ] dolch............... Dolch Word List\n",
            "  [ ] europarl_raw........ Sample European Parliament Proceedings Parallel\n",
            "                           Corpus\n",
            "  [ ] floresta............ Portuguese Treebank\n",
            "  [ ] framenet_v15........ FrameNet 1.5\n",
            "  [ ] framenet_v17........ FrameNet 1.7\n",
            "  [ ] gazetteers.......... Gazeteer Lists\n",
            "  [ ] genesis............. Genesis Corpus\n",
            "  [ ] gutenberg........... Project Gutenberg Selections\n",
            "  [ ] ieer................ NIST IE-ER DATA SAMPLE\n",
            "  [ ] inaugural........... C-Span Inaugural Address Corpus\n",
            "  [ ] indian.............. Indian Language POS-Tagged Corpus\n",
            "  [ ] jeita............... JEITA Public Morphologically Tagged Corpus (in\n",
            "                           ChaSen format)\n",
            "  [ ] kimmo............... PC-KIMMO Data Files\n",
            "  [ ] knbc................ KNB Corpus (Annotated blog corpus)\n",
            "  [ ] large_grammars...... Large context-free and feature-based grammars\n",
            "                           for parser comparison\n",
            "Hit Enter to continue: \n",
            "  [ ] lin_thesaurus....... Lin's Dependency Thesaurus\n",
            "  [ ] mac_morpho.......... MAC-MORPHO: Brazilian Portuguese news text with\n",
            "                           part-of-speech tags\n",
            "  [ ] machado............. Machado de Assis -- Obra Completa\n",
            "  [ ] masc_tagged......... MASC Tagged Corpus\n",
            "  [ ] maxent_ne_chunker... ACE Named Entity Chunker (Maximum entropy)\n",
            "  [ ] maxent_treebank_pos_tagger Treebank Part of Speech Tagger (Maximum entropy)\n",
            "  [ ] moses_sample........ Moses Sample Models\n",
            "  [ ] movie_reviews....... Sentiment Polarity Dataset Version 2.0\n",
            "  [ ] mte_teip5........... MULTEXT-East 1984 annotated corpus 4.0\n",
            "  [ ] mwa_ppdb............ The monolingual word aligner (Sultan et al.\n",
            "                           2015) subset of the Paraphrase Database.\n",
            "  [ ] names............... Names Corpus, Version 1.3 (1994-03-29)\n",
            "  [ ] nombank.1.0......... NomBank Corpus 1.0\n",
            "  [ ] nonbreaking_prefixes Non-Breaking Prefixes (Moses Decoder)\n",
            "  [ ] nps_chat............ NPS Chat\n",
            "  [ ] omw................. Open Multilingual Wordnet\n",
            "  [ ] opinion_lexicon..... Opinion Lexicon\n",
            "  [ ] panlex_swadesh...... PanLex Swadesh Corpora\n",
            "  [ ] paradigms........... Paradigm Corpus\n",
            "  [ ] pe08................ Cross-Framework and Cross-Domain Parser\n",
            "                           Evaluation Shared Task\n",
            "Hit Enter to continue: \n",
            "  [ ] perluniprops........ perluniprops: Index of Unicode Version 7.0.0\n",
            "                           character properties in Perl\n",
            "  [ ] pil................. The Patient Information Leaflet (PIL) Corpus\n",
            "  [ ] pl196x.............. Polish language of the XX century sixties\n",
            "  [ ] porter_test......... Porter Stemmer Test Files\n",
            "  [ ] ppattach............ Prepositional Phrase Attachment Corpus\n",
            "  [ ] problem_reports..... Problem Report Corpus\n",
            "  [ ] product_reviews_1... Product Reviews (5 Products)\n",
            "  [ ] product_reviews_2... Product Reviews (9 Products)\n",
            "  [ ] propbank............ Proposition Bank Corpus 1.0\n",
            "  [ ] pros_cons........... Pros and Cons\n",
            "  [ ] ptb................. Penn Treebank\n",
            "  [ ] qc.................. Experimental Data for Question Classification\n",
            "  [ ] reuters............. The Reuters-21578 benchmark corpus, ApteMod\n",
            "                           version\n",
            "  [ ] rslp................ RSLP Stemmer (Removedor de Sufixos da Lingua\n",
            "                           Portuguesa)\n",
            "  [ ] rte................. PASCAL RTE Challenges 1, 2, and 3\n",
            "  [ ] sample_grammars..... Sample Grammars\n",
            "  [ ] semcor.............. SemCor 3.0\n",
            "  [ ] senseval............ SENSEVAL 2 Corpus: Sense Tagged Text\n",
            "Hit Enter to continue: \n",
            "  [ ] sentence_polarity... Sentence Polarity Dataset v1.0\n",
            "  [ ] sentiwordnet........ SentiWordNet\n",
            "  [ ] shakespeare......... Shakespeare XML Corpus Sample\n",
            "  [ ] sinica_treebank..... Sinica Treebank Corpus Sample\n",
            "  [ ] smultron............ SMULTRON Corpus Sample\n",
            "  [ ] snowball_data....... Snowball Data\n",
            "  [ ] spanish_grammars.... Grammars for Spanish\n",
            "  [ ] state_union......... C-Span State of the Union Address Corpus\n",
            "  [ ] subjectivity........ Subjectivity Dataset v1.0\n",
            "  [ ] swadesh............. Swadesh Wordlists\n",
            "  [ ] switchboard......... Switchboard Corpus Sample\n",
            "  [ ] tagsets............. Help on Tagsets\n",
            "  [ ] timit............... TIMIT Corpus Sample\n",
            "  [ ] toolbox............. Toolbox Sample Files\n",
            "  [ ] treebank............ Penn Treebank Sample\n",
            "  [ ] twitter_samples..... Twitter Samples\n",
            "  [ ] udhr2............... Universal Declaration of Human Rights Corpus\n",
            "                           (Unicode Version)\n",
            "  [ ] udhr................ Universal Declaration of Human Rights Corpus\n",
            "  [ ] unicode_samples..... Unicode Samples\n",
            "  [ ] universal_tagset.... Mappings to the Universal Part-of-Speech Tagset\n",
            "Hit Enter to continue: \n",
            "  [ ] universal_treebanks_v20 Universal Treebanks Version 2.0\n",
            "  [ ] vader_lexicon....... VADER Sentiment Lexicon\n",
            "  [ ] verbnet3............ VerbNet Lexicon, Version 3.3\n",
            "  [ ] verbnet............. VerbNet Lexicon, Version 2.1\n",
            "  [ ] webtext............. Web Text Corpus\n",
            "  [ ] wmt15_eval.......... Evaluation data from WMT15\n",
            "  [ ] word2vec_sample..... Word2Vec Sample\n",
            "  [ ] wordnet............. WordNet\n",
            "  [ ] wordnet_ic.......... WordNet-InfoContent\n",
            "  [ ] words............... Word Lists\n",
            "  [ ] ycoe................ York-Toronto-Helsinki Parsed Corpus of Old\n",
            "                           English Prose\n",
            "\n",
            "Collections:\n",
            "  [P] all-corpora......... All the corpora\n",
            "  [P] all-nltk............ All packages available on nltk_data gh-pages\n",
            "                           branch\n",
            "  [P] all................. All packages\n",
            "  [P] book................ Everything used in the NLTK Book\n",
            "  [P] popular............. Popular packages\n",
            "  [ ] tests............... Packages for running tests\n",
            "  [ ] third-party......... Third-party data packages\n",
            "Hit Enter to continue: \n",
            "\n",
            "([*] marks installed packages; [P] marks partially installed collections)\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> x\n",
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> q\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iRKMeXM9bRvo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "41b940a9-a46b-48d1-c946-8a00b2a0638a"
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "q1AEE0rSbkIM",
        "colab": {}
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "token = []\n",
        "for i in range(len(data['text'])):\n",
        "  token.append(word_tokenize(data['text'][i]))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8sxAcWfywMKU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "876a4b9d-c8f3-46e8-b5aa-8097b81a6fb1"
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "34QX_Skzsce-",
        "colab": {}
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "for i in range(len(data['text'])):\n",
        "  filtered_sentence = [w for w in token[i] if not w in stop_words] \n",
        "  token[i] = filtered_sentence"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3wzY9-GscBu8",
        "colab": {}
      },
      "source": [
        "import string\n",
        "# remove all tokens that are not alphabetic\n",
        "words = []\n",
        "for i in range(len(data['text'])):\n",
        "  words.append([word for word in token[i] if word.isalpha()])\n",
        "  for j in range(len(words[i])):\n",
        "    words[i][j] = words[i][j].lower()"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iRC1noPdiBQ-",
        "colab": {}
      },
      "source": [
        "from nltk.stem import LancasterStemmer,PorterStemmer,SnowballStemmer\n",
        "Pst = PorterStemmer()"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "N7z0sv19lBtQ",
        "colab": {}
      },
      "source": [
        "for i in range(len(words)):\n",
        "  for j in range(len(words[i])):\n",
        "    words[i][j] = Pst.stem(words[i][j])"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XWDySkuYlfME",
        "colab": {}
      },
      "source": [
        "data2 = data\n",
        "for i in range(len(words)):\n",
        "  a = \"\"\n",
        "  for j in range(len(words[i])):\n",
        "     a=a + words[i][j] + \" \"\n",
        "  data2['text'][i] = a"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "a7RjxvYIzKuu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "085aa6f7-820c-4d35-9b2e-86886706b80c"
      },
      "source": [
        "data2.head() #After Preprocessing"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>airline_sentiment</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>neutral</td>\n",
              "      <td>virginamerica what dhepburn said</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>positive</td>\n",
              "      <td>virginamerica plu ad commerci experi tacki</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>neutral</td>\n",
              "      <td>virginamerica i today must mean i need take an...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>negative</td>\n",
              "      <td>virginamerica realli aggress blast obnoxi ente...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>negative</td>\n",
              "      <td>virginamerica realli big bad thing</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  airline_sentiment                                               text\n",
              "0           neutral                  virginamerica what dhepburn said \n",
              "1          positive        virginamerica plu ad commerci experi tacki \n",
              "2           neutral  virginamerica i today must mean i need take an...\n",
              "3          negative  virginamerica realli aggress blast obnoxi ente...\n",
              "4          negative                virginamerica realli big bad thing "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FL6bYvVV2Pzw",
        "colab": {}
      },
      "source": [
        "X = data2['text']\n",
        "Y = data2['airline_sentiment']"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFt5z0kK87B4",
        "colab_type": "text"
      },
      "source": [
        "4.    Vectorization\n",
        "5.    Fit and evaluate model using both type of vectorization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cn4foN3a1D0h",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "outputId": "09e0a5e2-551c-4476-9788-baf4fa5e58f3"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cv = CountVectorizer(lowercase=True, stop_words='english', min_df=2)\n",
        "X_cv = cv.fit_transform(X.values.astype('U'))\n",
        "X_cv.shape\n",
        "X_names = cv.get_feature_names()\n",
        "X_cv_df = pd.DataFrame(X_cv.toarray(),columns = X_names)\n",
        "X_cv_df.describe()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>aa</th>\n",
              "      <th>aadv</th>\n",
              "      <th>aadvantag</th>\n",
              "      <th>aafail</th>\n",
              "      <th>aarp</th>\n",
              "      <th>abandon</th>\n",
              "      <th>abc</th>\n",
              "      <th>abcnetwork</th>\n",
              "      <th>abil</th>\n",
              "      <th>abl</th>\n",
              "      <th>aboard</th>\n",
              "      <th>abq</th>\n",
              "      <th>abroad</th>\n",
              "      <th>absolut</th>\n",
              "      <th>absorb</th>\n",
              "      <th>absurd</th>\n",
              "      <th>abt</th>\n",
              "      <th>abus</th>\n",
              "      <th>abysm</th>\n",
              "      <th>ac</th>\n",
              "      <th>acc</th>\n",
              "      <th>accept</th>\n",
              "      <th>access</th>\n",
              "      <th>accid</th>\n",
              "      <th>accident</th>\n",
              "      <th>accommod</th>\n",
              "      <th>accompani</th>\n",
              "      <th>accomplish</th>\n",
              "      <th>accord</th>\n",
              "      <th>accordingli</th>\n",
              "      <th>account</th>\n",
              "      <th>accru</th>\n",
              "      <th>acct</th>\n",
              "      <th>accur</th>\n",
              "      <th>accus</th>\n",
              "      <th>achiev</th>\n",
              "      <th>acknowledg</th>\n",
              "      <th>acquir</th>\n",
              "      <th>acquisit</th>\n",
              "      <th>act</th>\n",
              "      <th>...</th>\n",
              "      <th>xna</th>\n",
              "      <th>xoxo</th>\n",
              "      <th>xt</th>\n",
              "      <th>xx</th>\n",
              "      <th>ya</th>\n",
              "      <th>yall</th>\n",
              "      <th>yard</th>\n",
              "      <th>yay</th>\n",
              "      <th>ye</th>\n",
              "      <th>yea</th>\n",
              "      <th>yeah</th>\n",
              "      <th>year</th>\n",
              "      <th>yearli</th>\n",
              "      <th>yell</th>\n",
              "      <th>yep</th>\n",
              "      <th>yest</th>\n",
              "      <th>yesterday</th>\n",
              "      <th>yike</th>\n",
              "      <th>yo</th>\n",
              "      <th>york</th>\n",
              "      <th>youareonyourown</th>\n",
              "      <th>young</th>\n",
              "      <th>younger</th>\n",
              "      <th>yousuck</th>\n",
              "      <th>youth</th>\n",
              "      <th>youv</th>\n",
              "      <th>yow</th>\n",
              "      <th>yr</th>\n",
              "      <th>yuck</th>\n",
              "      <th>yuma</th>\n",
              "      <th>yummi</th>\n",
              "      <th>yup</th>\n",
              "      <th>yvonn</th>\n",
              "      <th>yvr</th>\n",
              "      <th>yyz</th>\n",
              "      <th>zero</th>\n",
              "      <th>zkatcher</th>\n",
              "      <th>zone</th>\n",
              "      <th>zoom</th>\n",
              "      <th>zurich</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.013934</td>\n",
              "      <td>0.000137</td>\n",
              "      <td>0.000683</td>\n",
              "      <td>0.000137</td>\n",
              "      <td>0.000205</td>\n",
              "      <td>0.000273</td>\n",
              "      <td>0.000546</td>\n",
              "      <td>0.000205</td>\n",
              "      <td>0.000478</td>\n",
              "      <td>0.008265</td>\n",
              "      <td>0.000273</td>\n",
              "      <td>0.000615</td>\n",
              "      <td>0.000137</td>\n",
              "      <td>0.003552</td>\n",
              "      <td>0.000137</td>\n",
              "      <td>0.000956</td>\n",
              "      <td>0.000478</td>\n",
              "      <td>0.000205</td>\n",
              "      <td>0.000410</td>\n",
              "      <td>0.000273</td>\n",
              "      <td>0.000137</td>\n",
              "      <td>0.003757</td>\n",
              "      <td>0.002801</td>\n",
              "      <td>0.000410</td>\n",
              "      <td>0.000273</td>\n",
              "      <td>0.002391</td>\n",
              "      <td>0.000137</td>\n",
              "      <td>0.000273</td>\n",
              "      <td>0.000751</td>\n",
              "      <td>0.000273</td>\n",
              "      <td>0.005806</td>\n",
              "      <td>0.000137</td>\n",
              "      <td>0.000820</td>\n",
              "      <td>0.000751</td>\n",
              "      <td>0.000137</td>\n",
              "      <td>0.000205</td>\n",
              "      <td>0.000546</td>\n",
              "      <td>0.000137</td>\n",
              "      <td>0.000137</td>\n",
              "      <td>0.001230</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000205</td>\n",
              "      <td>0.000137</td>\n",
              "      <td>0.000137</td>\n",
              "      <td>0.000137</td>\n",
              "      <td>0.000820</td>\n",
              "      <td>0.002186</td>\n",
              "      <td>0.000137</td>\n",
              "      <td>0.000751</td>\n",
              "      <td>0.016462</td>\n",
              "      <td>0.000820</td>\n",
              "      <td>0.003074</td>\n",
              "      <td>0.009563</td>\n",
              "      <td>0.000137</td>\n",
              "      <td>0.001025</td>\n",
              "      <td>0.001161</td>\n",
              "      <td>0.000273</td>\n",
              "      <td>0.007240</td>\n",
              "      <td>0.000137</td>\n",
              "      <td>0.000956</td>\n",
              "      <td>0.000820</td>\n",
              "      <td>0.000137</td>\n",
              "      <td>0.000615</td>\n",
              "      <td>0.000137</td>\n",
              "      <td>0.000273</td>\n",
              "      <td>0.000205</td>\n",
              "      <td>0.000137</td>\n",
              "      <td>0.000137</td>\n",
              "      <td>0.002254</td>\n",
              "      <td>0.000137</td>\n",
              "      <td>0.000273</td>\n",
              "      <td>0.000137</td>\n",
              "      <td>0.000615</td>\n",
              "      <td>0.000137</td>\n",
              "      <td>0.000478</td>\n",
              "      <td>0.000956</td>\n",
              "      <td>0.002459</td>\n",
              "      <td>0.000205</td>\n",
              "      <td>0.000683</td>\n",
              "      <td>0.000137</td>\n",
              "      <td>0.000273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.125660</td>\n",
              "      <td>0.011688</td>\n",
              "      <td>0.026127</td>\n",
              "      <td>0.011688</td>\n",
              "      <td>0.014314</td>\n",
              "      <td>0.016528</td>\n",
              "      <td>0.023371</td>\n",
              "      <td>0.014314</td>\n",
              "      <td>0.021862</td>\n",
              "      <td>0.091290</td>\n",
              "      <td>0.016528</td>\n",
              "      <td>0.024787</td>\n",
              "      <td>0.011688</td>\n",
              "      <td>0.059494</td>\n",
              "      <td>0.011688</td>\n",
              "      <td>0.030910</td>\n",
              "      <td>0.021862</td>\n",
              "      <td>0.014314</td>\n",
              "      <td>0.023373</td>\n",
              "      <td>0.016528</td>\n",
              "      <td>0.011688</td>\n",
              "      <td>0.061180</td>\n",
              "      <td>0.052848</td>\n",
              "      <td>0.020241</td>\n",
              "      <td>0.016528</td>\n",
              "      <td>0.048838</td>\n",
              "      <td>0.011688</td>\n",
              "      <td>0.016528</td>\n",
              "      <td>0.027402</td>\n",
              "      <td>0.016528</td>\n",
              "      <td>0.078629</td>\n",
              "      <td>0.011688</td>\n",
              "      <td>0.028619</td>\n",
              "      <td>0.027402</td>\n",
              "      <td>0.011688</td>\n",
              "      <td>0.014314</td>\n",
              "      <td>0.023371</td>\n",
              "      <td>0.011688</td>\n",
              "      <td>0.011688</td>\n",
              "      <td>0.035044</td>\n",
              "      <td>...</td>\n",
              "      <td>0.014314</td>\n",
              "      <td>0.011688</td>\n",
              "      <td>0.011688</td>\n",
              "      <td>0.011688</td>\n",
              "      <td>0.030914</td>\n",
              "      <td>0.048143</td>\n",
              "      <td>0.011688</td>\n",
              "      <td>0.027402</td>\n",
              "      <td>0.131472</td>\n",
              "      <td>0.028619</td>\n",
              "      <td>0.055358</td>\n",
              "      <td>0.100093</td>\n",
              "      <td>0.011688</td>\n",
              "      <td>0.031994</td>\n",
              "      <td>0.034058</td>\n",
              "      <td>0.016528</td>\n",
              "      <td>0.084785</td>\n",
              "      <td>0.011688</td>\n",
              "      <td>0.036950</td>\n",
              "      <td>0.028619</td>\n",
              "      <td>0.011688</td>\n",
              "      <td>0.024787</td>\n",
              "      <td>0.011688</td>\n",
              "      <td>0.016528</td>\n",
              "      <td>0.014314</td>\n",
              "      <td>0.011688</td>\n",
              "      <td>0.011688</td>\n",
              "      <td>0.050224</td>\n",
              "      <td>0.011688</td>\n",
              "      <td>0.016528</td>\n",
              "      <td>0.011688</td>\n",
              "      <td>0.024787</td>\n",
              "      <td>0.011688</td>\n",
              "      <td>0.021862</td>\n",
              "      <td>0.030910</td>\n",
              "      <td>0.049529</td>\n",
              "      <td>0.014314</td>\n",
              "      <td>0.026127</td>\n",
              "      <td>0.011688</td>\n",
              "      <td>0.016528</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>3.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows × 4132 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                 aa          aadv  ...          zoom        zurich\n",
              "count  14640.000000  14640.000000  ...  14640.000000  14640.000000\n",
              "mean       0.013934      0.000137  ...      0.000137      0.000273\n",
              "std        0.125660      0.011688  ...      0.011688      0.016528\n",
              "min        0.000000      0.000000  ...      0.000000      0.000000\n",
              "25%        0.000000      0.000000  ...      0.000000      0.000000\n",
              "50%        0.000000      0.000000  ...      0.000000      0.000000\n",
              "75%        0.000000      0.000000  ...      0.000000      0.000000\n",
              "max        3.000000      1.000000  ...      1.000000      1.000000\n",
              "\n",
              "[8 rows x 4132 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "r9400ouyQWBh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "fc60a3ea-5642-4ae8-9cd1-3b16a4495d05"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "xT, xt, yT, yt = train_test_split(X_cv_df, Y, test_size=0.30, random_state=11)\n",
        "model = LogisticRegression()\n",
        "model.fit(xT, yT)\n",
        "model.predict(xt)\n",
        "pd.DataFrame(model.predict(xt))\n",
        "model.score(xT,yT)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8966627634660421"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "66_P79YLSuaP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "outputId": "a2da0c8e-3f6e-4cba-b140-4eac5ee17b96"
      },
      "source": [
        "res = pd.DataFrame(model.predict(xt))\n",
        "res"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4387</th>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4388</th>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4389</th>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4390</th>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4391</th>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4392 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "             0\n",
              "0     positive\n",
              "1     negative\n",
              "2     negative\n",
              "3     negative\n",
              "4     positive\n",
              "...        ...\n",
              "4387  negative\n",
              "4388  negative\n",
              "4389  negative\n",
              "4390  negative\n",
              "4391  negative\n",
              "\n",
              "[4392 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "x_awkdAgRjlx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "outputId": "aab8b5f7-1b73-4523-8d14-c741abe65d81"
      },
      "source": [
        "pd.DataFrame(model.predict_proba(xt))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.012397</td>\n",
              "      <td>0.012299</td>\n",
              "      <td>0.975304</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.990027</td>\n",
              "      <td>0.003772</td>\n",
              "      <td>0.006201</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.906792</td>\n",
              "      <td>0.066127</td>\n",
              "      <td>0.027081</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.998449</td>\n",
              "      <td>0.001496</td>\n",
              "      <td>0.000055</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.028212</td>\n",
              "      <td>0.026576</td>\n",
              "      <td>0.945212</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4387</th>\n",
              "      <td>0.597264</td>\n",
              "      <td>0.162959</td>\n",
              "      <td>0.239777</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4388</th>\n",
              "      <td>0.995104</td>\n",
              "      <td>0.004345</td>\n",
              "      <td>0.000551</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4389</th>\n",
              "      <td>0.637274</td>\n",
              "      <td>0.021400</td>\n",
              "      <td>0.341326</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4390</th>\n",
              "      <td>0.590242</td>\n",
              "      <td>0.244241</td>\n",
              "      <td>0.165517</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4391</th>\n",
              "      <td>0.996735</td>\n",
              "      <td>0.000122</td>\n",
              "      <td>0.003143</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4392 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "             0         1         2\n",
              "0     0.012397  0.012299  0.975304\n",
              "1     0.990027  0.003772  0.006201\n",
              "2     0.906792  0.066127  0.027081\n",
              "3     0.998449  0.001496  0.000055\n",
              "4     0.028212  0.026576  0.945212\n",
              "...        ...       ...       ...\n",
              "4387  0.597264  0.162959  0.239777\n",
              "4388  0.995104  0.004345  0.000551\n",
              "4389  0.637274  0.021400  0.341326\n",
              "4390  0.590242  0.244241  0.165517\n",
              "4391  0.996735  0.000122  0.003143\n",
              "\n",
              "[4392 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fz83dG6Y2pOy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ab5f0629-ced3-4271-e470-87169e137dfa"
      },
      "source": [
        "model.score(xt, yt)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7800546448087432"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OLD7TRQ0SL-H",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c3aea947-597c-44d2-82e9-2b210d4fc41b"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tv = TfidfVectorizer(lowercase=True, stop_words='english',min_df=2)\n",
        "tv_1 = tv.fit_transform(X)\n",
        "tv_1.shape"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(14640, 4132)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dbm8ERiG5fq6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "outputId": "ae79466a-dbd6-4a62-fca7-6502861a80c9"
      },
      "source": [
        "tv_1_names = tv.get_feature_names()\n",
        "tv_1_df = pd.DataFrame(tv_1.toarray(),columns= tv_1_names)\n",
        "tv_1_df.describe()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>aa</th>\n",
              "      <th>aadv</th>\n",
              "      <th>aadvantag</th>\n",
              "      <th>aafail</th>\n",
              "      <th>aarp</th>\n",
              "      <th>abandon</th>\n",
              "      <th>abc</th>\n",
              "      <th>abcnetwork</th>\n",
              "      <th>abil</th>\n",
              "      <th>abl</th>\n",
              "      <th>aboard</th>\n",
              "      <th>abq</th>\n",
              "      <th>abroad</th>\n",
              "      <th>absolut</th>\n",
              "      <th>absorb</th>\n",
              "      <th>absurd</th>\n",
              "      <th>abt</th>\n",
              "      <th>abus</th>\n",
              "      <th>abysm</th>\n",
              "      <th>ac</th>\n",
              "      <th>acc</th>\n",
              "      <th>accept</th>\n",
              "      <th>access</th>\n",
              "      <th>accid</th>\n",
              "      <th>accident</th>\n",
              "      <th>accommod</th>\n",
              "      <th>accompani</th>\n",
              "      <th>accomplish</th>\n",
              "      <th>accord</th>\n",
              "      <th>accordingli</th>\n",
              "      <th>account</th>\n",
              "      <th>accru</th>\n",
              "      <th>acct</th>\n",
              "      <th>accur</th>\n",
              "      <th>accus</th>\n",
              "      <th>achiev</th>\n",
              "      <th>acknowledg</th>\n",
              "      <th>acquir</th>\n",
              "      <th>acquisit</th>\n",
              "      <th>act</th>\n",
              "      <th>...</th>\n",
              "      <th>xna</th>\n",
              "      <th>xoxo</th>\n",
              "      <th>xt</th>\n",
              "      <th>xx</th>\n",
              "      <th>ya</th>\n",
              "      <th>yall</th>\n",
              "      <th>yard</th>\n",
              "      <th>yay</th>\n",
              "      <th>ye</th>\n",
              "      <th>yea</th>\n",
              "      <th>yeah</th>\n",
              "      <th>year</th>\n",
              "      <th>yearli</th>\n",
              "      <th>yell</th>\n",
              "      <th>yep</th>\n",
              "      <th>yest</th>\n",
              "      <th>yesterday</th>\n",
              "      <th>yike</th>\n",
              "      <th>yo</th>\n",
              "      <th>york</th>\n",
              "      <th>youareonyourown</th>\n",
              "      <th>young</th>\n",
              "      <th>younger</th>\n",
              "      <th>yousuck</th>\n",
              "      <th>youth</th>\n",
              "      <th>youv</th>\n",
              "      <th>yow</th>\n",
              "      <th>yr</th>\n",
              "      <th>yuck</th>\n",
              "      <th>yuma</th>\n",
              "      <th>yummi</th>\n",
              "      <th>yup</th>\n",
              "      <th>yvonn</th>\n",
              "      <th>yvr</th>\n",
              "      <th>yyz</th>\n",
              "      <th>zero</th>\n",
              "      <th>zkatcher</th>\n",
              "      <th>zone</th>\n",
              "      <th>zoom</th>\n",
              "      <th>zurich</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "      <td>14640.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.004016</td>\n",
              "      <td>0.000055</td>\n",
              "      <td>0.000285</td>\n",
              "      <td>0.000068</td>\n",
              "      <td>0.000093</td>\n",
              "      <td>0.000118</td>\n",
              "      <td>0.000259</td>\n",
              "      <td>0.000094</td>\n",
              "      <td>0.000213</td>\n",
              "      <td>0.002949</td>\n",
              "      <td>0.000124</td>\n",
              "      <td>0.000262</td>\n",
              "      <td>0.000070</td>\n",
              "      <td>0.001492</td>\n",
              "      <td>0.000065</td>\n",
              "      <td>0.000435</td>\n",
              "      <td>0.000193</td>\n",
              "      <td>0.000094</td>\n",
              "      <td>0.000184</td>\n",
              "      <td>0.000122</td>\n",
              "      <td>0.000062</td>\n",
              "      <td>0.001419</td>\n",
              "      <td>0.001028</td>\n",
              "      <td>0.000202</td>\n",
              "      <td>0.000108</td>\n",
              "      <td>0.000935</td>\n",
              "      <td>0.000051</td>\n",
              "      <td>0.000135</td>\n",
              "      <td>0.000326</td>\n",
              "      <td>0.000135</td>\n",
              "      <td>0.001975</td>\n",
              "      <td>0.000060</td>\n",
              "      <td>0.000333</td>\n",
              "      <td>0.000318</td>\n",
              "      <td>0.000052</td>\n",
              "      <td>0.000107</td>\n",
              "      <td>0.000254</td>\n",
              "      <td>0.000051</td>\n",
              "      <td>0.000057</td>\n",
              "      <td>0.000524</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000115</td>\n",
              "      <td>0.000090</td>\n",
              "      <td>0.000061</td>\n",
              "      <td>0.000097</td>\n",
              "      <td>0.000401</td>\n",
              "      <td>0.000922</td>\n",
              "      <td>0.000061</td>\n",
              "      <td>0.000398</td>\n",
              "      <td>0.006038</td>\n",
              "      <td>0.000408</td>\n",
              "      <td>0.001238</td>\n",
              "      <td>0.003068</td>\n",
              "      <td>0.000068</td>\n",
              "      <td>0.000413</td>\n",
              "      <td>0.000561</td>\n",
              "      <td>0.000123</td>\n",
              "      <td>0.002565</td>\n",
              "      <td>0.000099</td>\n",
              "      <td>0.000351</td>\n",
              "      <td>0.000315</td>\n",
              "      <td>0.000071</td>\n",
              "      <td>0.000263</td>\n",
              "      <td>0.000062</td>\n",
              "      <td>0.000144</td>\n",
              "      <td>0.000085</td>\n",
              "      <td>0.000067</td>\n",
              "      <td>0.000068</td>\n",
              "      <td>0.000762</td>\n",
              "      <td>0.000068</td>\n",
              "      <td>0.000160</td>\n",
              "      <td>0.000073</td>\n",
              "      <td>0.000302</td>\n",
              "      <td>0.000055</td>\n",
              "      <td>0.000186</td>\n",
              "      <td>0.000409</td>\n",
              "      <td>0.000879</td>\n",
              "      <td>0.000108</td>\n",
              "      <td>0.000275</td>\n",
              "      <td>0.000054</td>\n",
              "      <td>0.000110</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.036735</td>\n",
              "      <td>0.004724</td>\n",
              "      <td>0.010934</td>\n",
              "      <td>0.005821</td>\n",
              "      <td>0.006673</td>\n",
              "      <td>0.007155</td>\n",
              "      <td>0.011356</td>\n",
              "      <td>0.006572</td>\n",
              "      <td>0.009940</td>\n",
              "      <td>0.033732</td>\n",
              "      <td>0.007666</td>\n",
              "      <td>0.010746</td>\n",
              "      <td>0.005947</td>\n",
              "      <td>0.026527</td>\n",
              "      <td>0.005588</td>\n",
              "      <td>0.014169</td>\n",
              "      <td>0.008947</td>\n",
              "      <td>0.006601</td>\n",
              "      <td>0.010194</td>\n",
              "      <td>0.007464</td>\n",
              "      <td>0.005279</td>\n",
              "      <td>0.023858</td>\n",
              "      <td>0.019751</td>\n",
              "      <td>0.010172</td>\n",
              "      <td>0.006582</td>\n",
              "      <td>0.019564</td>\n",
              "      <td>0.004353</td>\n",
              "      <td>0.008304</td>\n",
              "      <td>0.012096</td>\n",
              "      <td>0.008368</td>\n",
              "      <td>0.026947</td>\n",
              "      <td>0.005177</td>\n",
              "      <td>0.011768</td>\n",
              "      <td>0.011752</td>\n",
              "      <td>0.004459</td>\n",
              "      <td>0.007518</td>\n",
              "      <td>0.011080</td>\n",
              "      <td>0.004387</td>\n",
              "      <td>0.004895</td>\n",
              "      <td>0.015264</td>\n",
              "      <td>...</td>\n",
              "      <td>0.008034</td>\n",
              "      <td>0.007883</td>\n",
              "      <td>0.005238</td>\n",
              "      <td>0.008791</td>\n",
              "      <td>0.015220</td>\n",
              "      <td>0.020653</td>\n",
              "      <td>0.005191</td>\n",
              "      <td>0.015043</td>\n",
              "      <td>0.050866</td>\n",
              "      <td>0.014628</td>\n",
              "      <td>0.023071</td>\n",
              "      <td>0.032971</td>\n",
              "      <td>0.005801</td>\n",
              "      <td>0.013162</td>\n",
              "      <td>0.016983</td>\n",
              "      <td>0.007440</td>\n",
              "      <td>0.030961</td>\n",
              "      <td>0.008903</td>\n",
              "      <td>0.012784</td>\n",
              "      <td>0.011052</td>\n",
              "      <td>0.006111</td>\n",
              "      <td>0.010760</td>\n",
              "      <td>0.005307</td>\n",
              "      <td>0.008762</td>\n",
              "      <td>0.005944</td>\n",
              "      <td>0.005749</td>\n",
              "      <td>0.005937</td>\n",
              "      <td>0.016880</td>\n",
              "      <td>0.005830</td>\n",
              "      <td>0.009790</td>\n",
              "      <td>0.006221</td>\n",
              "      <td>0.012652</td>\n",
              "      <td>0.004679</td>\n",
              "      <td>0.008575</td>\n",
              "      <td>0.013438</td>\n",
              "      <td>0.017880</td>\n",
              "      <td>0.007657</td>\n",
              "      <td>0.010546</td>\n",
              "      <td>0.004600</td>\n",
              "      <td>0.006662</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>0.916388</td>\n",
              "      <td>0.404156</td>\n",
              "      <td>0.478854</td>\n",
              "      <td>0.519895</td>\n",
              "      <td>0.601484</td>\n",
              "      <td>0.476797</td>\n",
              "      <td>0.654287</td>\n",
              "      <td>0.481474</td>\n",
              "      <td>0.640949</td>\n",
              "      <td>0.789578</td>\n",
              "      <td>0.613823</td>\n",
              "      <td>0.556236</td>\n",
              "      <td>0.516476</td>\n",
              "      <td>0.931357</td>\n",
              "      <td>0.532080</td>\n",
              "      <td>0.543635</td>\n",
              "      <td>0.520886</td>\n",
              "      <td>0.538095</td>\n",
              "      <td>0.723398</td>\n",
              "      <td>0.540891</td>\n",
              "      <td>0.468766</td>\n",
              "      <td>0.742987</td>\n",
              "      <td>0.575665</td>\n",
              "      <td>0.688679</td>\n",
              "      <td>0.449456</td>\n",
              "      <td>0.713934</td>\n",
              "      <td>0.397083</td>\n",
              "      <td>0.641906</td>\n",
              "      <td>0.570025</td>\n",
              "      <td>0.679814</td>\n",
              "      <td>0.607029</td>\n",
              "      <td>0.454377</td>\n",
              "      <td>0.519989</td>\n",
              "      <td>0.515092</td>\n",
              "      <td>0.382857</td>\n",
              "      <td>0.592316</td>\n",
              "      <td>0.648002</td>\n",
              "      <td>0.376746</td>\n",
              "      <td>0.418793</td>\n",
              "      <td>0.616282</td>\n",
              "      <td>...</td>\n",
              "      <td>0.589015</td>\n",
              "      <td>0.791297</td>\n",
              "      <td>0.448179</td>\n",
              "      <td>0.964427</td>\n",
              "      <td>0.759402</td>\n",
              "      <td>0.712503</td>\n",
              "      <td>0.459653</td>\n",
              "      <td>0.828971</td>\n",
              "      <td>0.910571</td>\n",
              "      <td>0.724694</td>\n",
              "      <td>0.750291</td>\n",
              "      <td>0.691994</td>\n",
              "      <td>0.515021</td>\n",
              "      <td>0.571481</td>\n",
              "      <td>0.805655</td>\n",
              "      <td>0.484645</td>\n",
              "      <td>0.751352</td>\n",
              "      <td>0.964427</td>\n",
              "      <td>0.812610</td>\n",
              "      <td>0.486224</td>\n",
              "      <td>0.543814</td>\n",
              "      <td>0.548203</td>\n",
              "      <td>0.469292</td>\n",
              "      <td>0.611962</td>\n",
              "      <td>0.459771</td>\n",
              "      <td>0.517727</td>\n",
              "      <td>0.587871</td>\n",
              "      <td>0.581116</td>\n",
              "      <td>0.511401</td>\n",
              "      <td>0.741416</td>\n",
              "      <td>0.565975</td>\n",
              "      <td>0.741460</td>\n",
              "      <td>0.414210</td>\n",
              "      <td>0.443130</td>\n",
              "      <td>0.593640</td>\n",
              "      <td>0.448268</td>\n",
              "      <td>0.624928</td>\n",
              "      <td>0.455539</td>\n",
              "      <td>0.409715</td>\n",
              "      <td>0.460871</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows × 4132 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                 aa          aadv  ...          zoom        zurich\n",
              "count  14640.000000  14640.000000  ...  14640.000000  14640.000000\n",
              "mean       0.004016      0.000055  ...      0.000054      0.000110\n",
              "std        0.036735      0.004724  ...      0.004600      0.006662\n",
              "min        0.000000      0.000000  ...      0.000000      0.000000\n",
              "25%        0.000000      0.000000  ...      0.000000      0.000000\n",
              "50%        0.000000      0.000000  ...      0.000000      0.000000\n",
              "75%        0.000000      0.000000  ...      0.000000      0.000000\n",
              "max        0.916388      0.404156  ...      0.409715      0.460871\n",
              "\n",
              "[8 rows x 4132 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cGDRhn1-3jVq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "21f322b7-0520-4d12-eb3c-75fe23e6cf76"
      },
      "source": [
        "x_train,x_test,y_train,y_test = train_test_split(tv_1_df,Y,test_size = 0.30, random_state = 11)\n",
        "model.fit(x_train,y_train)\n",
        "ypredict = model.predict(x_test)\n",
        "model.score(x_train,y_train)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8534348165495707"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ef7UBYvd5UWN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0e923ada-699e-4052-e286-558eea22a423"
      },
      "source": [
        "model.score(x_test, y_test)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.788023679417122"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YZbBSuY15mv0",
        "colab": {}
      },
      "source": [
        "Algo = []\n",
        "train = []\n",
        "test = []\n",
        "recall = []\n",
        "roc = []"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3ea05SZqYxXU",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import recall_score,precision_score,roc_auc_score\n",
        "Algo.append('Logistic Regression')\n",
        "train.append(model.score(x_train,y_train))\n",
        "test.append(model.score(x_test,y_test))\n",
        "#recall.append(recall_score(ypredict,y_test),average = 'micro')\n",
        "#roc.append(roc_auc_score(ypredict,y_test))\n"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BA6Wnj6leTof",
        "colab": {}
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gzpqaBo9eYKS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "416db74e-aeef-464f-db1f-fb90761ad584"
      },
      "source": [
        "model_decisiontree = DecisionTreeClassifier()\n",
        "model_decisiontree.fit(x_train,y_train)\n",
        "ypredict_decisiontree = model_decisiontree.predict(x_test)\n",
        "print(model_decisiontree.tree_.max_depth)\n",
        "#model_decisiontree.score(x_train,y_train) Overfitting of training set\n",
        "#model_decisiontree.score(x_test,y_test)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "206\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "z--gQM-jfAbv",
        "colab": {}
      },
      "source": [
        "model_pruned = DecisionTreeClassifier(criterion='entropy',max_depth=3,random_state=5)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uYgAO1IAfzwb",
        "colab": {}
      },
      "source": [
        "model_pruned.fit(x_train,y_train)\n",
        "ypredict = model_pruned.predict(x_test)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CzoqttaUhb3-",
        "colab": {}
      },
      "source": [
        "Algo.append('Decision Tree using Entropy')\n",
        "train.append(model_pruned.score(x_train,y_train))\n",
        "test.append(model_pruned.score(x_test,y_test))\n",
        "#recall.append(recall_score(ypredict,y_test))\n",
        "#roc.append(roc_auc_score(ypredict,y_test))"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rTlw55KcikDH",
        "colab": {}
      },
      "source": [
        "model = DecisionTreeClassifier(criterion='gini',max_depth=3,random_state=5)\n",
        "#model = DecisionTreemodel()\n",
        "model.fit(x_train,y_train)\n",
        "ypredict = model.predict(x_test)\n",
        "Algo.append('Decision Tree using Gini')\n",
        "train.append(model.score(x_train,y_train))\n",
        "test.append(model.score(x_test,y_test))\n",
        "#recall.append(recall_score(ypredict,y_test))\n",
        "#roc.append(roc_auc_score(ypredict,y_test))"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EeVNkkV_3KNk",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "model = RandomForestClassifier(n_estimators = 100)\n",
        "model.fit(x_train,y_train)\n",
        "ypredict = model.predict(x_test)\n",
        "Algo.append('Random Forest')\n",
        "train.append(model.score(x_train,y_train))\n",
        "test.append(model.score(x_test,y_test))"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6OdbMsUJ4hr",
        "colab_type": "text"
      },
      "source": [
        "Following Classifier have low accuracy and are time consuming hence commented"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PDa-5f_AjKNM",
        "colab": {}
      },
      "source": [
        "#from sklearn.ensemble import BaggingClassifier\n",
        "#model = BaggingClassifier(n_estimators=100,max_samples=0.7,bootstrap=True,oob_score=True,random_state=11)\n",
        "#model.fit(x_train,y_train)\n",
        "#ypredict = model.predict(x_test)\n",
        "#Algo.append('Bagging model')\n",
        "#train.append(model.score(x_train,y_train))\n",
        "#test.append(model.score(x_test,y_test))\n",
        "#recall.append(recall_score(ypredict,y_test))\n",
        "#roc.append(roc_auc_score(ypredict,y_test))"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0vkMwFBPjurL",
        "colab": {}
      },
      "source": [
        "#from sklearn.ensemble import AdaBoostClassifier\n",
        "#model = AdaBoostClassifier(n_estimators=200,learning_rate=0.1,random_state=11)\n",
        "#model.fit(x_train,y_train)\n",
        "#ypredict = model.predict(x_test)\n",
        "#Algo.append('AdaBoost model')\n",
        "#train.append(model.score(x_train,y_train))\n",
        "#test.append(model.score(x_test,y_test))\n",
        "#recall.append(recall_score(ypredict,y_test))\n",
        "#roc.append(roc_auc_score(ypredict,y_test))"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FwM1BGoHkIVH",
        "colab": {}
      },
      "source": [
        "#from sklearn.ensemble import GradientBoostingClassifier\n",
        "#model = GradientBoostingClassifier(n_estimators=200,random_state=11)\n",
        "#model.fit(x_train,y_train)\n",
        "#ypredict = model.predict(x_test)\n",
        "#Algo.append('Gradient Boosting model')\n",
        "#train.append(model.score(x_train,y_train))\n",
        "#test.append(model.score(x_test,y_test))\n",
        "#recall.append(recall_score(ypredict,y_test))\n",
        "#roc.append(roc_auc_score(ypredict,y_test))"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XLL5Hb8JkcKm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "719ecc45-7efb-4d44-9c79-deefa4d7f7ff"
      },
      "source": [
        "results = pd.DataFrame()\n",
        "results['Model'] = Algo\n",
        "results['Training Accuracy'] = train\n",
        "results['Testing Accuracy'] = test\n",
        "#results['Recall'] = recall\n",
        "#results['ROC AUC Score'] = roc\n",
        "results = results.set_index('Model')\n",
        "results"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Training Accuracy</th>\n",
              "      <th>Testing Accuracy</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Model</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Logistic Regression</th>\n",
              "      <td>0.853435</td>\n",
              "      <td>0.788024</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Decision Tree using Entropy</th>\n",
              "      <td>0.678962</td>\n",
              "      <td>0.681922</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Decision Tree using Gini</th>\n",
              "      <td>0.683743</td>\n",
              "      <td>0.686931</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Random Forest</th>\n",
              "      <td>0.993657</td>\n",
              "      <td>0.757058</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                             Training Accuracy  Testing Accuracy\n",
              "Model                                                           \n",
              "Logistic Regression                   0.853435          0.788024\n",
              "Decision Tree using Entropy           0.678962          0.681922\n",
              "Decision Tree using Gini              0.683743          0.686931\n",
              "Random Forest                         0.993657          0.757058"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIZJ5dwIFoaS",
        "colab_type": "text"
      },
      "source": [
        "6. The preprocessing of the text data is an essential step as it makes the raw text ready for mining, i.e., it becomes easier to extract information from the text and apply machine learning algorithms to it. If we skip this step then there is a higher chance that you are working with noisy and inconsistent data.Vectorization is to Convert the data into Numerical Matrix in which ML Algorithms can easily be applied\n",
        "\n",
        "In this Case we find Logistic Regression to be most effective with 78.8% Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fuc7nuF4AZcl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(len(Y)):\n",
        "  if Y[i] == 'positive':\n",
        "    Y[i] = 0.0\n",
        "  if Y[i] == 'negative':\n",
        "    Y[i] = 1.0\n",
        "  if Y[i] == 'neutral':\n",
        "    Y[i] = 2.0"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAMzbS5eIE_3",
        "colab_type": "text"
      },
      "source": [
        "# **7.    If applicable apply any Classification based Machine learning/Deep Learning framework to have better understanding of the Use Case.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euDsqO6U2Y2F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clf = RandomForestClassifier(\n",
        "    n_estimators=50,\n",
        "    criterion='gini',\n",
        "    max_depth=5,\n",
        "    min_samples_split=2,\n",
        "    min_samples_leaf=1,\n",
        "    min_weight_fraction_leaf=0.0,\n",
        "    max_features='auto',\n",
        "    max_leaf_nodes=None,\n",
        "    min_impurity_decrease=0.0,\n",
        "    min_impurity_split=None,\n",
        "    bootstrap=True,\n",
        "    oob_score=False,\n",
        "    n_jobs=-1,\n",
        "    random_state=0,\n",
        "    verbose=0,\n",
        "    warm_start=False,\n",
        "    class_weight='balanced'\n",
        ")"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKtri3ZZ2Y2K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#from sklearn.model_selection import StratifiedKFold\n",
        "#cv = StratifiedKFold(n_splits=5, random_state=123, shuffle=True)\n",
        "#results = pd.DataFrame(columns=['training_score', 'test_score'])\n",
        "#fprs, tprs, scores = [], [], []\n",
        "    \n",
        "#for (train1, test1), i in zip(cv.split(X_cv_df, Y), range(5)):\n",
        "#    clf.fit(X_cv_df.iloc[train1], Y.iloc[train1])\n",
        "#    train.append(clf.score(x_train,y_train))\n",
        "#    test.append(clf.score(x_test,y_test))\n",
        "#    Algo.append('RFCV')\n",
        "#train.append(model.score(x_train,y_train))"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6J2yFLz22Y2O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "ts_split= TimeSeriesSplit(n_splits=5)\n",
        "for train_index, test_index in ts_split.split(X_cv_df):\n",
        "        X_train, X_test = X_cv_df[:len(train_index)], X_cv_df[len(train_index): (len(train_index)+len(test_index))]\n",
        "        y_train, y_test = Y[:len(train_index)].values.ravel(), Y[len(train_index): (len(train_index)+len(test_index))].values.ravel()"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_r0x5Tf-Xzv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "0d3a1fd1-55ae-486c-c189-8cf318c7424e"
      },
      "source": [
        "X_train.info()"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 12200 entries, 0 to 12199\n",
            "Columns: 4132 entries, aa to zurich\n",
            "dtypes: int64(4132)\n",
            "memory usage: 384.6 MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQv08IyS2Y2d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train =np.array(X_train)\n",
        "X_test =np.array(X_test)\n",
        "\n",
        "X_tr_t = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])\n",
        "X_tst_t = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPRBlgHV9l3w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a5a18baa-88f0-4558-b0ec-8c16212b82f0"
      },
      "source": [
        "for i in range(len(y_train)):\n",
        "  if y_train[i] == 'positive':\n",
        "    y_train[i] = 0.0\n",
        "  if y_train[i] == 'negative':\n",
        "    y_train[i] = 1.0\n",
        "  if y_train[i] == 'neutral':\n",
        "    y_train[i] = 2.0\n",
        "for i in range(len(y_test)):\n",
        "  if y_test[i] == 'positive':\n",
        "    y_test[i] = 0.0\n",
        "  if y_test[i] == 'negative':\n",
        "    y_test[i] = 1.0\n",
        "  if y_test[i] == 'neutral':\n",
        "    y_test[i] = 2.0\n",
        "y_train"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2.0, 0.0, 2.0, ..., 0.0, 1.0, 0.0], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-oeKMFmKBUlF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = np.asarray(X_train).astype(np.float32)\n",
        "y_train = np.asarray(y_train).astype(np.float32)\n",
        "y_test = np.asarray(y_test).astype(np.float32)\n",
        "X_tr_t = np.asarray(X_tr_t).astype(np.float32)\n",
        "X_tst_t = np.asarray(X_tst_t).astype(np.float32)"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QpxUnXma2Y2i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 905
        },
        "outputId": "188f53cb-bc92-43e0-acdd-fb68a72c4864"
      },
      "source": [
        "#import tensorflow as tf\n",
        "#from tensorflow import keras\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from keras.layers import LSTM,Dense,Dropout\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "import keras.backend as K\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.optimizers import Adam\n",
        "from keras.models import load_model\n",
        "from keras.layers import LSTM\n",
        "K.clear_session()\n",
        "model_lstm = Sequential()\n",
        "model_lstm.add(LSTM(16, input_shape=(1, X_train.shape[1]), activation='relu', return_sequences=False))\n",
        "model_lstm.add(Dense(1))\n",
        "model_lstm.compile(loss='mean_squared_error', optimizer='adam')\n",
        "early_stop = EarlyStopping(monitor='loss', patience=5, verbose=1)\n",
        "history_model_lstm = model_lstm.fit(X_tr_t, y_train, epochs=25, batch_size=8, verbose=1, shuffle=False, callbacks=[early_stop])"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "1525/1525 [==============================] - 5s 3ms/step - loss: 0.4006\n",
            "Epoch 2/25\n",
            "1525/1525 [==============================] - 5s 3ms/step - loss: 0.2578\n",
            "Epoch 3/25\n",
            "1525/1525 [==============================] - 5s 3ms/step - loss: 0.2221\n",
            "Epoch 4/25\n",
            "1525/1525 [==============================] - 5s 3ms/step - loss: 0.1941\n",
            "Epoch 5/25\n",
            "1525/1525 [==============================] - 5s 3ms/step - loss: 0.1701\n",
            "Epoch 6/25\n",
            "1525/1525 [==============================] - 5s 3ms/step - loss: 0.1496\n",
            "Epoch 7/25\n",
            "1525/1525 [==============================] - 5s 3ms/step - loss: 0.1333\n",
            "Epoch 8/25\n",
            "1525/1525 [==============================] - 5s 3ms/step - loss: 0.1200\n",
            "Epoch 9/25\n",
            "1525/1525 [==============================] - 5s 3ms/step - loss: 0.1093\n",
            "Epoch 10/25\n",
            "1525/1525 [==============================] - 5s 3ms/step - loss: 0.1005\n",
            "Epoch 11/25\n",
            "1525/1525 [==============================] - 5s 3ms/step - loss: 0.0926\n",
            "Epoch 12/25\n",
            "1525/1525 [==============================] - 5s 3ms/step - loss: 0.0868\n",
            "Epoch 13/25\n",
            "1525/1525 [==============================] - 5s 3ms/step - loss: 0.0807\n",
            "Epoch 14/25\n",
            "1525/1525 [==============================] - 5s 3ms/step - loss: 0.0762\n",
            "Epoch 15/25\n",
            "1525/1525 [==============================] - 5s 3ms/step - loss: 0.0722\n",
            "Epoch 16/25\n",
            "1525/1525 [==============================] - 5s 3ms/step - loss: 0.0684\n",
            "Epoch 17/25\n",
            "1525/1525 [==============================] - 5s 3ms/step - loss: 0.0645\n",
            "Epoch 18/25\n",
            "1525/1525 [==============================] - 5s 3ms/step - loss: 0.0616\n",
            "Epoch 19/25\n",
            "1525/1525 [==============================] - 5s 3ms/step - loss: 0.0591\n",
            "Epoch 20/25\n",
            "1525/1525 [==============================] - 5s 3ms/step - loss: 0.0570\n",
            "Epoch 21/25\n",
            "1525/1525 [==============================] - 5s 3ms/step - loss: 0.0541\n",
            "Epoch 22/25\n",
            "1525/1525 [==============================] - 5s 3ms/step - loss: 0.0523\n",
            "Epoch 23/25\n",
            "1525/1525 [==============================] - 5s 3ms/step - loss: 0.0502\n",
            "Epoch 24/25\n",
            "1525/1525 [==============================] - 5s 3ms/step - loss: 0.0490\n",
            "Epoch 25/25\n",
            "1525/1525 [==============================] - 5s 3ms/step - loss: 0.0475\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "biwE-XCi2Y2n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b819a138-dd0b-4a96-ec90-74088c9f7892"
      },
      "source": [
        "y_pred_test_lstm = model_lstm.predict(X_tst_t)\n",
        "y_train_pred_lstm = model_lstm.predict(X_tr_t)\n",
        "print(\"The R2 score on the Train set is:\\t{:0.3f}\".format(r2_score(y_train, y_train_pred_lstm)))\n",
        "r2_train = r2_score(y_train, y_train_pred_lstm)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The R2 score on the Train set is:\t0.883\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}